{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative extension analysis notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Authors**\n",
    "\n",
    "- Jérémy Bensoussan\n",
    "- Ekaterina Kryukova\n",
    "- Jules Triomphe\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "While the paper examines the exposure hypothesis for all topics, we propose to analyze and compare political and food related tweets. To do so, we plan to obtain egos, alters and their timelines from Twitter’s API by generating $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$. Then, we intend to identify egos’ retweets using official RT, classify tweets by topics based on hashtags (and keywords if the dataset is lacking content) and create a follower/followee graph. To calculate the probability of retweeting alters’ tweets by egos we will use a more solid approach then what is described in the paper “Differences in the Mechanics of Information Diffusion Across Topics” where the probability is equal to the number of users that were k times exposed to a hashtag and retweeted before the ($k+1$)-th exposure, divided by the number of users that were k-times exposed to the hashtag. Finally, we plan to visualize results as well as analyze the probability by breaking down users based on betweenness, clustering coefficient and number of followees.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. Is there a significant difference between the probability of retweeting when the tweet is about food and when it is about politics ?\n",
    "2. Is there a significant difference in the number of times a tweet is retweeted depending on whether it is about food or politics ?\n",
    "3. Is there a relation between user betweenness, size of cluster or number of friends with the retweet probabilities ?\n",
    "\n",
    "## Proposed dataset\n",
    "\n",
    "Self-collected (with the Twitter API) ego and alter timelines with all tweet fields from the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) and all user fields except for `profile_image_url`.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data collection\n",
    "\n",
    "We will sign up for Twitter’s API to collect data. We will generate $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$ as in the paper and use the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) to collect active and public user information with all tweet fields and all user fields except for profile_image_url.\n",
    "\n",
    "### Building the network\n",
    "\n",
    "We will use networkx to build a directed network of followers  in which nodes are users (egos and alters) and edges are the following relationships (without the following relationships among alters). Next, we will build another network where relationships among alters of active egos are included.\n",
    "\n",
    "### Calculating retweet probability\n",
    "\n",
    "For each ego, we will count the number of followees who have retweeted a post (exposures) on a certain topic at a certain date. We will get the information like this: an ego $i$ was exposed to $200$ posts about this topic only once, among which $i$ retweeted $50$ (probability is $50/200 = 25\\%$); at the same time, $i$ was exposed to $100$ posts about this topic twice, among which $i$ retweeted $50$ ($probability = 50\\%$); … Finally, we will calculate a sequence of probability for each ego. (Same procedure as in the paper.) If there is sufficient data, we will apply a t-test to identify whether the distribution of retweets for each exposure count is significantly different from one topic to the other. We will also try to compare the distribution of retweet probabilities between topics.\n",
    "\n",
    "### Community detection\n",
    "\n",
    "We will use the second ego networks to compute clustering coefficients and betweenness of the active egos.\n",
    "\n",
    "### Data analysis\n",
    "\n",
    "We will compare the retweet probabilities based on betweenness, number of followers and clustering for each topic, much like in *Figure 6* in the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d8f9ab97aa40>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-formatting\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# UserID range\n",
    "LOWER_ID_N = 0\n",
    "UPPER_ID_N = int(3e9)\n",
    "\n",
    "# UserID number\n",
    "N_UID_PER_REQUEST = int(3e4)\n",
    "N_UID_REQUESTS = 3\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Choose whether to generate new UserIDs\n",
    "CREATE_NEW_UIDs = False\n",
    "\n",
    "# Choose whether to collect user data\n",
    "COLLECT_USER_DATA = False\n",
    "# Select the batch to query if collecting user data\n",
    "REQUEST_NUMBER = 2\n",
    "# Define behaviour depending on the run number\n",
    "FIRST_RUN = False\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Data folder location\n",
    "DATA_FOLDER = \"./data/\"\n",
    "# UIDs file name\n",
    "UIDS_FILE = DATA_FOLDER + \"uids.csv\"\n",
    "USERS_FILE = DATA_FOLDER + \"users.csv\"\n",
    "# File containing the bearer token\n",
    "BEARER_TOKEN = DATA_FOLDER + \"bearer_token.auth\"\n",
    "\n",
    "# API endpoints\n",
    "API_USERS_ENDPOINT = \"https://api.twitter.com/2/users?ids=\"\n",
    "API_USER_FIELDS = \"user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified,withheld\"\n",
    "API_TWEET_FIELDS = \"tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,non_public_metrics,public_metrics,organic_metrics,promoted_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "API_USER_TIMELINE_ENDPOINT = (\n",
    "    \"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id=\"\n",
    ")\n",
    "\n",
    "# Random seed\n",
    "SEED = 30\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data collection\n",
    "\n",
    "In this part, we will generate random user IDs and collect their respective user information if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate UIDs\n",
    "if CREATE_NEW_UIDs:\n",
    "    uids = pd.DataFrame(\n",
    "        np.array(\n",
    "            random.sample(\n",
    "                range(LOWER_ID_N, UPPER_ID_N), N_UID_PER_REQUEST * N_UID_REQUESTS\n",
    "            )\n",
    "        ).reshape(N_UID_PER_REQUEST, N_UID_REQUESTS)\n",
    "    )\n",
    "    uids.to_csv(UIDS_FILE, index=False)\n",
    "else:\n",
    "    uids = pd.read_csv(UIDS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bearer token\n",
    "with open(BEARER_TOKEN, \"r\") as file:\n",
    "    token = file.readline().strip(\"\\n\")\n",
    "\n",
    "# Define authentication header\n",
    "headers = {\"Authorization\": \"Bearer \" + token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_reset(r):\n",
    "    ts = int(r.headers[\"x-rate-limit-reset\"])\n",
    "    print(\"Current time: {} (UTC)\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    print(\n",
    "        \"Waiting until {} (UTC) for the rate limit to reset.\".format(\n",
    "            datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "    )\n",
    "    time.sleep((datetime.fromtimestamp(ts) - datetime.now()).total_seconds())\n",
    "    print(\"Resuming user data collection\")\n",
    "\n",
    "\n",
    "def get_user_data(req, headers=headers, wait=True):\n",
    "    # Query the user data\n",
    "    r = requests.get(req, headers=headers)\n",
    "    if wait & int(r.headers[\"x-rate-limit-remaining\"]) == 0:\n",
    "        wait_for_reset(r)\n",
    "\n",
    "        # Query the user data\n",
    "        r = requests.get(req, headers=headers)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_user_data_df(r):\n",
    "    df = pd.DataFrame(\n",
    "        r.json()[\"data\"],\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"username\",\n",
    "            \"name\",\n",
    "            \"protected\",\n",
    "            \"withheld\",\n",
    "            \"verified\",\n",
    "            \"created_at\",\n",
    "            \"location\",\n",
    "            \"public_metrics\",\n",
    "            \"description\",\n",
    "            \"url\",\n",
    "            \"entities\",\n",
    "            \"pinned_tweet_id\",\n",
    "        ],\n",
    "        # Replace NaNs by empty strings to facilitate pre-processing\n",
    "    ).fillna(\"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Get user data\n",
    "if COLLECT_USER_DATA:\n",
    "    for i in trange(N_UID_PER_REQUEST // 100):\n",
    "        # Get 100 UserIDs (limit per request as defined by Twitter)\n",
    "        users = uids.values[i * 100 : (i + 1) * 100, REQUEST_NUMBER]\n",
    "        # Define the request URL\n",
    "        req = (\n",
    "            API_USERS_ENDPOINT\n",
    "            + \",\".join([str(user) for user in users])\n",
    "            + \"&\"\n",
    "            + API_USER_FIELDS\n",
    "            + \"&\"\n",
    "            + API_TWEET_FIELDS\n",
    "        )\n",
    "\n",
    "        # Create the dataframe on the first iteration\n",
    "        if i == 0:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers)\n",
    "\n",
    "            # If the rate limit is not maximal, then wait for the reset to occur\n",
    "            # (max 15 minutes)\n",
    "            if int(r.headers[\"x-rate-limit-remaining\"]) != 299:\n",
    "                wait_for_reset(r)\n",
    "\n",
    "                # Query the user data\n",
    "                r = get_user_data(req, headers=headers)\n",
    "\n",
    "            df = get_user_data_df(r)\n",
    "        # Append to existing dataframe on other iterations\n",
    "        # but do not wait for reset for the last iteration\n",
    "        elif i == 299:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers, wait=False)\n",
    "            # Append new data to existing dataframe\n",
    "            df = df.append(get_user_data_df(r))\n",
    "        else:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers)\n",
    "            # Append new data to existing dataframe\n",
    "            df = df.append(get_user_data_df(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "\n",
    "def get_key_val(x, key):\n",
    "    \"\"\"Get dictionary value from key if it exists, otherwise return an empty string.\"\"\"\n",
    "    if key in x:\n",
    "        return x[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_public_metrics(df):\n",
    "    \"\"\"Extract the data from the public_metrics column\"\"\"\n",
    "    for metric in [\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]:\n",
    "        df[metric] = df.public_metrics.apply(lambda x: get_key_val(x, metric))\n",
    "    df.pop(\"public_metrics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_entities(df):\n",
    "    \"\"\"Extract the data from the entity column\"\"\"\n",
    "    for entity in [\"url\", \"description\"]:\n",
    "        df[\"entities_\" + entity] = df.entities.apply(lambda x: get_key_val(x, entity))\n",
    "    df.pop(\"entities\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    df = get_public_metrics(df)\n",
    "    df = get_entities(df)\n",
    "    df = df.astype({\"id\": int, \"protected\": bool, \"verified\": bool})\n",
    "    print(\"Number of valid users: {:,}\".format(df.shape[0]))\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot safely convert passed user dtype of bool for object dtyped data in column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('bool') according to the rule 'safe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a801165041eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFIRST_RUN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSERS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"protected\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verified'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCOLLECT_USER_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0muser_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_venvs/2020-ada/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_venvs/2020-ada/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_venvs/2020-ada/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_venvs/2020-ada/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2146\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot safely convert passed user dtype of bool for object dtyped data in column 3"
     ]
    }
   ],
   "source": [
    "if not FIRST_RUN:\n",
    "    user_data = pd.read_csv(USERS_FILE, dtype={\"id\": int, \"protected\": bool, 'verified': bool}, lineterminator=\"\\n\")\n",
    "    if COLLECT_USER_DATA:\n",
    "        user_data = user_data.append(df)\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    # Save data to disk\n",
    "    user_data.to_csv(USERS_FILE, index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Total number of valid users: {:,}\".format(user_data.shape[0]))\n",
    "print(\n",
    "    \"Number of public accounts: {:,}\".format(\n",
    "        user_data[user_data.protected.astype(bool)].shape[0]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"--> With at least one tweet: {:,}\".format(\n",
    "        user_data[user_data.protected.astype(bool) & (user_data.tweet_count > 0)].shape[\n",
    "            0\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(r.headers[\"x-rate-limit-remaining\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = (\n",
    "    API_USERS_ENDPOINT\n",
    "    + \"783214,15994119,1320117356\"\n",
    "    + \"&\"\n",
    "    + API_USER_FIELDS\n",
    "    + \"&\"\n",
    "    + API_TWEET_FIELDS\n",
    ")\n",
    "print(req)\n",
    "r = requests.get(req, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    r.json()[\"data\"],\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"username\",\n",
    "        \"name\",\n",
    "        \"protected\",\n",
    "        \"withheld\",\n",
    "        \"verified\",\n",
    "        \"created_at\",\n",
    "        \"location\",\n",
    "        \"public_metrics\",\n",
    "        \"description\",\n",
    "        \"url\",\n",
    "        \"entities\",\n",
    "        \"pinned_tweet_id\",\n",
    "    ],\n",
    ").fillna(\"\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2020-ada",
   "language": "python",
   "name": "2020-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
