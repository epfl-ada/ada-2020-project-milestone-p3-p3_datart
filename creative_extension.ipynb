{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative extension analysis notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Authors**\n",
    "\n",
    "- Jérémy Bensoussan\n",
    "- Ekaterina Kryukova\n",
    "- Jules Triomphe\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "While the paper examines the exposure hypothesis for all topics, we propose to analyze and compare political and food related tweets. To do so, we plan to obtain egos, alters and their timelines from Twitter’s API by generating $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$. Then, we intend to identify egos’ retweets using official RT, classify tweets by topics based on hashtags (and keywords if the dataset is lacking content) and create a follower/followee graph. To calculate the probability of retweeting alters’ tweets by egos we will use a more solid approach then what is described in the paper “Differences in the Mechanics of Information Diffusion Across Topics” where the probability is equal to the number of users that were k times exposed to a hashtag and retweeted before the ($k+1$)-th exposure, divided by the number of users that were k-times exposed to the hashtag. Finally, we plan to visualize results as well as analyze the probability by breaking down users based on betweenness, clustering coefficient and number of followees.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. Is there a significant difference between the probability of retweeting when the tweet is about food and when it is about politics ?\n",
    "2. Is there a significant difference in the number of times a tweet is retweeted depending on whether it is about food or politics ?\n",
    "3. Is there a relation between user betweenness, size of cluster or number of friends with the retweet probabilities ?\n",
    "\n",
    "## Proposed dataset\n",
    "\n",
    "Self-collected (with the Twitter API) ego and alter timelines with all tweet fields from the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) and all user fields except for `profile_image_url`.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data collection\n",
    "\n",
    "We will sign up for Twitter’s API to collect data. We will generate $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$ as in the paper and use the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) to collect active and public user information with all tweet fields and all user fields except for profile_image_url.\n",
    "\n",
    "### Building the network\n",
    "\n",
    "We will use networkx to build a directed network of followers  in which nodes are users (egos and alters) and edges are the following relationships (without the following relationships among alters). Next, we will build another network where relationships among alters of active egos are included.\n",
    "\n",
    "### Calculating retweet probability\n",
    "\n",
    "For each ego, we will count the number of followees who have retweeted a post (exposures) on a certain topic at a certain date. We will get the information like this: an ego $i$ was exposed to $200$ posts about this topic only once, among which $i$ retweeted $50$ (probability is $50/200 = 25\\%$); at the same time, $i$ was exposed to $100$ posts about this topic twice, among which $i$ retweeted $50$ ($probability = 50\\%$); … Finally, we will calculate a sequence of probability for each ego. (Same procedure as in the paper.) If there is sufficient data, we will apply a t-test to identify whether the distribution of retweets for each exposure count is significantly different from one topic to the other. We will also try to compare the distribution of retweet probabilities between topics.\n",
    "\n",
    "### Community detection\n",
    "\n",
    "We will use the second ego networks to compute clustering coefficients and betweenness of the active egos.\n",
    "\n",
    "### Data analysis\n",
    "\n",
    "We will compare the retweet probabilities based on betweenness, number of followers and clustering for each topic, much like in *Figure 6* in the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup automatic formatting (requires the `nb-black` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-formatting\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control center\n",
    "\n",
    "**This is the control center. All operations are decided here to avoid memory overflow and excessive computation times. This notebook should be run FROM THE TOP once these parameters have been set.** If in doubt, ask Jules ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# UserID range\n",
    "LOWER_ID_N = 0\n",
    "UPPER_ID_N = int(3e9)\n",
    "\n",
    "# UserID number\n",
    "N_UID_PER_REQUEST = int(3e4)\n",
    "N_UID_REQUESTS = 3\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Choose whether to generate new UserIDs\n",
    "CREATE_NEW_UIDs = False\n",
    "\n",
    "# Choose whether to collect user data\n",
    "COLLECT_USER_DATA = False\n",
    "# Select the batch to query if collecting user data\n",
    "REQUEST_NUMBER = 2\n",
    "# Define behaviour depending on the run number.\n",
    "# If this is True then COLLECT_USER_DATA must be True\n",
    "FIRST_RUN = False\n",
    "\n",
    "# Chooser whether to create user subset files\n",
    "CREATE_USER_SUBSETS = False\n",
    "\n",
    "# Choose whether to create/reset data pull status\n",
    "# This also controls whether the PUBLIC_USERS_TIMELINES_FILE is overwritten\n",
    "CREATE_DATA_PULL_STATUS = False\n",
    "# Select the users to pull (from public_users_w_tweets)\n",
    "PULL_START = 0\n",
    "PULL_END = PULL_START + 1000\n",
    "# Choose whether to pull new data and save it\n",
    "PULL_NEW_TIMELINE_DATA = True\n",
    "N_RUNS_TIMELINE_DATA = 100\n",
    "# Choose whether to save newly pulled data\n",
    "SAVE_PULLED_DATA = True\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Data folder location\n",
    "DATA_FOLDER = \"./data/\"\n",
    "# UIDs\n",
    "UIDS_FILE = DATA_FOLDER + \"uids.csv\"\n",
    "# User files\n",
    "USERS_FOLDER = DATA_FOLDER + \"users/\"\n",
    "USERS_FILE = USERS_FOLDER + \"users.csv\"\n",
    "PUBLIC_USERS_FILE = USERS_FOLDER + \"public_users.csv\"\n",
    "PUBLIC_USERS_W_TWEETS_FILE = USERS_FOLDER + \"public_users_w_tweets.csv\"\n",
    "# Pulled data\n",
    "PUBLIC_USERS_PULL_STATUS_FILE = (\n",
    "    DATA_FOLDER + f\"public_users_pull_status_{PULL_START:05}_to_{PULL_END:05}.csv\"\n",
    ")\n",
    "# Timeline files\n",
    "TIMELINES_FOLDER = DATA_FOLDER + \"timelines/\"\n",
    "PUBLIC_USERS_TIMELINES_FILE = (\n",
    "    TIMELINES_FOLDER + f\"public_users_timelines_{PULL_START:05}_to_{PULL_END:05}.csv\"\n",
    ")\n",
    "# File containing the bearer token\n",
    "BEARER_TOKEN = DATA_FOLDER + \"bearer_token.auth\"\n",
    "\n",
    "# API endpoints\n",
    "API_USERS_ENDPOINT = \"https://api.twitter.com/2/users?ids=\"\n",
    "API_USER_FIELDS = \"user.fields=created_at,id,protected,public_metrics,username,verified\"\n",
    "# API_USER_FIELDS = \"user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified,withheld\"\n",
    "API_TWEET_FIELDS = \"tweet.fields=\"\n",
    "# API_TWEET_FIELDS = \"tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,non_public_metrics,public_metrics,organic_metrics,promoted_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "API_V1_RATE_LIMITS = \"https://api.twitter.com/1.1/application/rate_limit_status.json?resources=application,statuses,followers,friends\"\n",
    "API_USER_TIMELINE_ENDPOINT = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"\n",
    "\n",
    "# Random seed\n",
    "SEED = 30\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the folders if they do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [DATA_FOLDER, USERS_FOLDER, TIMELINES_FOLDER]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data collection\n",
    "\n",
    "In this part, we will generate random user IDs and collect their respective user information if they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UID generation\n",
    "\n",
    "Let's create random UIDs in the 0-3 billion range as discussed in the abstract.\n",
    "\n",
    "We reshape them to simplify queries due to Twitter's API's rate limits.\n",
    "\n",
    "If they have already been generated, we load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_NEW_UIDs:\n",
    "    uids = pd.DataFrame(\n",
    "        np.array(\n",
    "            random.sample(\n",
    "                range(LOWER_ID_N, UPPER_ID_N), N_UID_PER_REQUEST * N_UID_REQUESTS\n",
    "            )\n",
    "        ).reshape(N_UID_PER_REQUEST, N_UID_REQUESTS)\n",
    "    )\n",
    "    uids.to_csv(UIDS_FILE, index=False)\n",
    "else:\n",
    "    uids = pd.read_csv(UIDS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token load\n",
    "\n",
    "To query Twitter's API, we need a bearer token which we load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bearer token\n",
    "with open(BEARER_TOKEN, \"r\") as file:\n",
    "    token = file.readline().strip(\"\\n\")\n",
    "\n",
    "# Define authentication header\n",
    "headers = {\"Authorization\": \"Bearer \" + token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User data collection\n",
    "\n",
    "In this section, we will get user data from Twitter's API.\n",
    "\n",
    "First we define a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_reset(r):\n",
    "    print(\n",
    "        \"Current time: {} (UTC)\".format(datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    # Get reset time (Unix format)\n",
    "    ts = int(r.headers[\"x-rate-limit-reset\"])\n",
    "    ts_str = datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Compute difference between current time and reset time\n",
    "    sleep_time = (datetime.fromtimestamp(ts) - datetime.utcnow()).total_seconds()\n",
    "    if sleep_time > 0:\n",
    "        print(\"Waiting until {} (UTC) for the rate limit to reset.\".format(ts_str))\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        print(\"Reset time was: {} (UTC)\".format(ts_str))\n",
    "    print(\"Resuming user data collection\")\n",
    "\n",
    "\n",
    "def get_user_data(req, headers=headers, wait=True):\n",
    "    # Query the user data\n",
    "    r = requests.get(req, headers=headers)\n",
    "    if wait & (int(r.headers[\"x-rate-limit-remaining\"]) == 0):\n",
    "        wait_for_reset(r)\n",
    "\n",
    "        # Query the user data\n",
    "        r = requests.get(req, headers=headers)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_user_data_df(r):\n",
    "    df = pd.DataFrame(\n",
    "        r.json()[\"data\"],\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"username\",\n",
    "            \"protected\",\n",
    "            \"verified\",\n",
    "            \"created_at\",\n",
    "            \"public_metrics\",\n",
    "        ],\n",
    "        # Replace NaNs by empty strings to facilitate pre-processing\n",
    "    ).fillna(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's query the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user data\n",
    "if COLLECT_USER_DATA:\n",
    "    for i in trange(N_UID_PER_REQUEST // 100):\n",
    "        # Get 100 UserIDs (limit per request as defined by Twitter)\n",
    "        users = uids.values[i * 100 : (i + 1) * 100, REQUEST_NUMBER]\n",
    "        # Define the request URL\n",
    "        req = (\n",
    "            API_USERS_ENDPOINT\n",
    "            + \",\".join([str(user) for user in users])\n",
    "            + \"&\"\n",
    "            + API_USER_FIELDS\n",
    "            + \"&\"\n",
    "            + API_TWEET_FIELDS\n",
    "        )\n",
    "\n",
    "        # Create the dataframe on the first iteration\n",
    "        if i == 0:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req)\n",
    "\n",
    "            print(r)\n",
    "\n",
    "            # If the rate limit is not maximal, then wait for the reset to occur\n",
    "            # (max 15 minutes)\n",
    "            if int(r.headers[\"x-rate-limit-remaining\"]) != 299:\n",
    "                wait_for_reset(r)\n",
    "\n",
    "                # Query the user data\n",
    "                r = get_user_data(req)\n",
    "\n",
    "            raw_user_data = get_user_data_df(r)\n",
    "        # Append to existing dataframe on other iterations\n",
    "        # but do not wait for reset for the last iteration\n",
    "        elif i == 299:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, wait=False)\n",
    "            # Append new data to existing dataframe\n",
    "            raw_user_data = raw_user_data.append(get_user_data_df(r))\n",
    "        else:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req)\n",
    "            # Append new data to existing dataframe\n",
    "            raw_user_data = raw_user_data.append(get_user_data_df(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few important data points we will need to the next parts so we extract them here along with any others they are grouped with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "\n",
    "def get_key_val(x, key):\n",
    "    \"\"\"Get dictionary value from key if it exists, otherwise return an empty string.\"\"\"\n",
    "    if key in x:\n",
    "        return x[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_public_metrics(df):\n",
    "    \"\"\"Extract the data from the public_metrics column\"\"\"\n",
    "    for metric in [\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]:\n",
    "        df[metric] = df.public_metrics.apply(lambda x: get_key_val(x, metric))\n",
    "    df.pop(\"public_metrics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_entities(df):\n",
    "    \"\"\"Extract the data from the entity column\"\"\"\n",
    "    for entity in [\"url\", \"description\"]:\n",
    "        df[\"entities_\" + entity] = df.entities.apply(lambda x: get_key_val(x, entity))\n",
    "    df.pop(\"entities\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    raw_user_data = get_public_metrics(raw_user_data)\n",
    "    #     raw_user_data = get_entities(raw_user_data)\n",
    "    raw_user_data = raw_user_data.astype(\n",
    "        {\n",
    "            \"id\": int,\n",
    "            \"username\": str,\n",
    "            \"protected\": bool,\n",
    "            \"verified\": bool,\n",
    "            \"created_at\": str,\n",
    "            \"followers_count\": int,\n",
    "            \"following_count\": int,\n",
    "            \"tweet_count\": int,\n",
    "            \"listed_count\": int,\n",
    "        }\n",
    "    )\n",
    "    print(\"Number of valid users: {:,}\".format(raw_user_data.shape[0]))\n",
    "    raw_user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need all of the user data available for the next parts, so we append the generated data (if any) to pre-existing user data and we save the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid users: 33,511\n"
     ]
    }
   ],
   "source": [
    "# Load user data if it exists\n",
    "if os.path.isfile(USERS_FILE) and not FIRST_RUN:\n",
    "    user_data = pd.read_csv(\n",
    "        USERS_FILE,\n",
    "        dtype={\n",
    "            \"id\": int,\n",
    "            \"username\": str,\n",
    "            \"protected\": bool,\n",
    "            \"verified\": bool,\n",
    "            \"created_at\": str,\n",
    "            \"followers_count\": int,\n",
    "            \"following_count\": int,\n",
    "            \"tweet_count\": int,\n",
    "            \"listed_count\": int,\n",
    "        },\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "    if COLLECT_USER_DATA:\n",
    "        user_data = user_data.append(raw_user_data)\n",
    "else:\n",
    "    user_data = raw_user_data\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    # Save data to disk\n",
    "    user_data.to_csv(USERS_FILE, index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Total number of valid users: {:,}\".format(user_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User subset definition\n",
    "\n",
    "We define and save groups of users to facilitate data manipulation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user subsets\n",
    "if CREATE_USER_SUBSETS:\n",
    "    # Public users\n",
    "    public_users = user_data[~user_data.protected].copy()\n",
    "    # Public users with tweets\n",
    "    # Tweet count includes retweets and deleted tweets\n",
    "    public_users_w_tweets = user_data[\n",
    "        ~user_data.protected & (user_data.tweet_count > 0)\n",
    "    ].copy()\n",
    "\n",
    "    print(\"Number of public users: {:,}\".format(public_users.shape[0]))\n",
    "    print(\n",
    "        \"Number of public users with tweets: {:,}\".format(\n",
    "            public_users_w_tweets.shape[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    public_users.to_csv(PUBLIC_USERS_FILE, index=False)\n",
    "    public_users_w_tweets.to_csv(PUBLIC_USERS_W_TWEETS_FILE, index=False)\n",
    "\n",
    "elif os.path.isfile(PUBLIC_USERS_W_TWEETS_FILE):\n",
    "    public_users_w_tweets = pd.read_csv(\n",
    "        PUBLIC_USERS_W_TWEETS_FILE,\n",
    "        dtype={\n",
    "            \"id\": int,\n",
    "            \"username\": str,\n",
    "            \"protected\": bool,\n",
    "            \"verified\": bool,\n",
    "            \"created_at\": str,\n",
    "            \"followers_count\": int,\n",
    "            \"following_count\": int,\n",
    "            \"tweet_count\": int,\n",
    "            \"listed_count\": int,\n",
    "        },\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT, os.strerror(errno.ENOENT), PUBLIC_USERS_W_TWEETS_FILE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pull status generation\n",
    "\n",
    "As there are many queries to make, we create here a dataframe to be able to keep track of what data was already pulled and what data still needs to be pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pull status dataframe\n",
    "if CREATE_DATA_PULL_STATUS:\n",
    "    # Use public metrics to define limits\n",
    "    user_data_pull_status = public_users_w_tweets[[\"id\", \"tweet_count\"]][\n",
    "        PULL_START:PULL_END\n",
    "    ].copy()\n",
    "\n",
    "    # Define parameters for API queries\n",
    "    user_data_pull_status[\"timeline_lowest_id\"] = 0\n",
    "    user_data_pull_status[\"timeline_tweets_pulled\"] = 0\n",
    "\n",
    "    # Change column order for easier visualization\n",
    "    user_data_pull_status = user_data_pull_status[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"timeline_lowest_id\",\n",
    "            \"timeline_tweets_pulled\",\n",
    "            \"tweet_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Set id column to index\n",
    "    user_data_pull_status = user_data_pull_status.set_index(\"id\")\n",
    "    # Save to file\n",
    "    user_data_pull_status.to_csv(PUBLIC_USERS_PULL_STATUS_FILE)\n",
    "\n",
    "elif os.path.isfile(PUBLIC_USERS_PULL_STATUS_FILE):\n",
    "    user_data_pull_status = pd.read_csv(\n",
    "        PUBLIC_USERS_PULL_STATUS_FILE,\n",
    "        dtype=int,\n",
    "        index_col=\"id\",\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT, os.strerror(errno.ENOENT), PUBLIC_USERS_PULL_STATUS_FILE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User timeline collection\n",
    "\n",
    "As part of our analysis, we need to collect users' timelines. This is what we do here.\n",
    "\n",
    "First we define a few helper functions whose names are pretty explicit, then we move on to actually query the data before saving it along with the data pull status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_timeline_rate_limit():\n",
    "    r = requests.get(API_V1_RATE_LIMITS, headers=headers)\n",
    "    remaining = r.json()[\"resources\"][\"statuses\"][\"/statuses/user_timeline\"][\n",
    "        \"remaining\"\n",
    "    ]\n",
    "    # Get Unix timestamp\n",
    "    reset_ts = r.json()[\"resources\"][\"statuses\"][\"/statuses/user_timeline\"][\"reset\"]\n",
    "    # Convert to string\n",
    "    reset_time = datetime.fromtimestamp(reset_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return remaining, reset_time\n",
    "\n",
    "\n",
    "def get_initial_timeline_df():\n",
    "    return pd.DataFrame(\n",
    "        columns=[\n",
    "            \"user_id\",\n",
    "            \"id\",\n",
    "            \"user_mentions\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_user_timeline_df(r):\n",
    "    df = pd.DataFrame(\n",
    "        r.json(),\n",
    "        columns=[\n",
    "            \"user_id\",\n",
    "            # User profile\n",
    "            \"user\",\n",
    "            \"id\",\n",
    "            # This feature contains the user mentions\n",
    "            \"entities\",\n",
    "            # This feature is expected to be NaN\n",
    "            \"user_mentions\",\n",
    "        ],\n",
    "        # Replace NaNs by empty strings to facilitate pre-processing\n",
    "    ).fillna(\"\")\n",
    "    # Fill in user_id with tweet UserID\n",
    "    df.user_id = df.user.apply(lambda x: x[\"id\"])\n",
    "\n",
    "    def get_user_mentions_ids(x):\n",
    "        users_mentioned = x[\"user_mentions\"]\n",
    "        user_mentions = []\n",
    "        for i in range(len(users_mentioned)):\n",
    "            user_mentions.append(users_mentioned[i][\"id\"])\n",
    "        return \";\".join(str(x) for x in user_mentions)\n",
    "\n",
    "    df.user_mentions = df.entities.apply(lambda x: get_user_mentions_ids(x))\n",
    "    df.drop(columns=[\"user\", \"entities\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_user_timeline(query_n, user_id, max_id, count):\n",
    "    req = API_USER_TIMELINE_ENDPOINT\n",
    "    params = {\n",
    "        \"user_id\": str(user_id),\n",
    "        \"count\": str(count),\n",
    "        \"include_rts\": \"1\",\n",
    "    }\n",
    "    if max_id > 0:\n",
    "        params.update({\"max_id\": str(max_id - 1)})\n",
    "\n",
    "    r = requests.get(req, headers=headers, params=params)\n",
    "    df = get_user_timeline_df(r)\n",
    "\n",
    "    n_tweets_pulled = len(r.json())\n",
    "    if n_tweets_pulled < count:\n",
    "        print(r.url)\n",
    "        print(\n",
    "            \"Query {:,} -- \".format(query_n + 1).ljust(15)\n",
    "            + \"User {}: got {:,} tweets instead of {:,}.\".format(\n",
    "                user_id, n_tweets_pulled, count\n",
    "            )\n",
    "        )\n",
    "        lowest_id = -1\n",
    "    if df.shape[0] > 0:\n",
    "        lowest_id = int(df.id.min())\n",
    "\n",
    "    return df, lowest_id, n_tweets_pulled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined our helper functions, we now create an empty dataframe for our user timeline data and query the API for as much data as possible until we hit the rate limit (similar sections are run multiple times (days...) to query all of the necessary data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pull sequence...\n",
      "\n",
      "No more tweets to pull!\n",
      "\n",
      "Loop is done!\n"
     ]
    }
   ],
   "source": [
    "tmp_user_timeline_data = get_initial_timeline_df()\n",
    "\n",
    "if PULL_NEW_TIMELINE_DATA:\n",
    "    for n in range(N_RUNS_TIMELINE_DATA):\n",
    "        print(\"Starting pull sequence...\")\n",
    "\n",
    "        tmp_user_timeline_data = get_initial_timeline_df()\n",
    "\n",
    "        # Get the number of available queries and rate limit reset time\n",
    "        query_quota, reset_time = get_user_timeline_rate_limit()\n",
    "\n",
    "        # Get users with tweets left to pull\n",
    "        user_timelines_to_pull = user_data_pull_status[\n",
    "            (user_data_pull_status.tweet_count > 0)\n",
    "            & (\n",
    "                user_data_pull_status.timeline_tweets_pulled\n",
    "                < user_data_pull_status.tweet_count\n",
    "            )\n",
    "            # The API limits pulls to the 3.2k most recent tweets\n",
    "            & (user_data_pull_status.timeline_tweets_pulled < 3200)\n",
    "        ]\n",
    "\n",
    "        skip_sleep_time = user_timelines_to_pull.shape[0] < query_quota\n",
    "        n_queries = min(query_quota, user_timelines_to_pull.shape[0])\n",
    "        if n_queries > 0:\n",
    "            print(\"Executing {:,} queries.\".format(n_queries))\n",
    "        else:\n",
    "            print(\"\\nNo more tweets to pull!\\n\")\n",
    "            break\n",
    "        for query_n in trange(n_queries):\n",
    "\n",
    "            # Get query parameters\n",
    "            user_id = user_timelines_to_pull.index[query_n]\n",
    "            max_id = user_timelines_to_pull.loc[user_id, \"timeline_lowest_id\"]\n",
    "            # A 200-tweet limit is set by Twitter per request\n",
    "            count = min(\n",
    "                min(user_timelines_to_pull.loc[user_id, \"tweet_count\"], 3200)\n",
    "                - user_timelines_to_pull.loc[user_id, \"timeline_tweets_pulled\"],\n",
    "                200,\n",
    "            )\n",
    "\n",
    "            # Get user timeline data and statistics\n",
    "            raw_user_timeline_data, lowest_id, n_tweets_pulled = get_user_timeline(\n",
    "                query_n, user_id, max_id, count\n",
    "            )\n",
    "\n",
    "            # Append to existing user timeline data\n",
    "            tmp_user_timeline_data = tmp_user_timeline_data.append(\n",
    "                raw_user_timeline_data\n",
    "            )\n",
    "\n",
    "            # Update pull status\n",
    "            user_data_pull_status.loc[user_id, \"timeline_lowest_id\"] = lowest_id\n",
    "            if user_timelines_to_pull.loc[user_id, \"timeline_tweets_pulled\"] == 0:\n",
    "                user_data_pull_status.loc[user_id, \"timeline_tweets_pulled\"] = count\n",
    "            else:\n",
    "                user_data_pull_status.loc[user_id, \"timeline_tweets_pulled\"] += count\n",
    "\n",
    "        print(\"Next reset time: {} (UTC)\".format(reset_time))\n",
    "\n",
    "        print(\"\\nPull is done!\\n\")\n",
    "\n",
    "        # Define user timelines dataframe\n",
    "        if os.path.isfile(PUBLIC_USERS_TIMELINES_FILE) and not CREATE_DATA_PULL_STATUS:\n",
    "            user_timeline_data = pd.read_csv(PUBLIC_USERS_TIMELINES_FILE)\n",
    "            user_timeline_data = user_timeline_data.append(tmp_user_timeline_data)\n",
    "        else:\n",
    "            user_timeline_data = tmp_user_timeline_data\n",
    "\n",
    "        print(\n",
    "            \"Number of collected tweets: {:,} ({:,} unique) out of {:,}.\\nNumber of unique users: {:,} (out of {:,}).\".format(\n",
    "                user_timeline_data.shape[0],\n",
    "                len(np.unique(user_timeline_data.id.values)),\n",
    "                np.sum(user_data_pull_status.tweet_count.values),\n",
    "                len(np.unique(user_timeline_data.user_id.values)),\n",
    "                public_users_w_tweets.shape[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if SAVE_PULLED_DATA:\n",
    "            # Save data to disk\n",
    "            user_timeline_data.to_csv(PUBLIC_USERS_TIMELINES_FILE, index=False)\n",
    "            user_data_pull_status.to_csv(PUBLIC_USERS_PULL_STATUS_FILE)\n",
    "            print(\"Data saved!\")\n",
    "\n",
    "        # Get new reset time\n",
    "        query_quota, reset_time = get_user_timeline_rate_limit()\n",
    "\n",
    "        # Sleep until rate limit reset\n",
    "        sleep_time = (\n",
    "            datetime.strptime(reset_time, \"%Y-%m-%d %H:%M:%S\") - datetime.utcnow()\n",
    "        ).total_seconds()\n",
    "\n",
    "        if sleep_time > 0 and not skip_sleep_time:\n",
    "            print(\n",
    "                \"Waiting for {:,.0f} seconds to continue (until {} (UTC)).\\n\".format(\n",
    "                    sleep_time, reset_time\n",
    "                )\n",
    "            )\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(\"Loop is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-15-6aaf1f276005>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status[user_data_pull_status.tweet_count > 0].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_initial_followers_df().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\n",
    "    \"https://api.twitter.com/1.1/followers/ids.json?user_id=555533734&cursor=-1&count=311\",\n",
    "    headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(\n",
    "    r.json(),\n",
    "    columns=[\n",
    "        \"user_id\",\n",
    "        \"ids\",\n",
    "        \"next_cursor\",\n",
    "    ],\n",
    "    dtype=int,\n",
    ").fillna(\"\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r.json()[\"ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(user_timeline_data.id.values).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.path.isfile(PUBLIC_USERS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    columns=[\n",
    "        \"user_id\",\n",
    "        \"user\",\n",
    "        \"id\",\n",
    "        \"created_at\",\n",
    "        \"text\",\n",
    "        \"in_reply_to_status_id\",\n",
    "        \"in_reply_to_user_id\",\n",
    "        \"source\",\n",
    "        \"truncated\",\n",
    "        \"coordinates\",\n",
    "        \"place\",\n",
    "        \"is_quote_status\",\n",
    "        \"quoted_status_id\",\n",
    "        \"quoted_status\",\n",
    "        \"quote_count\",\n",
    "        \"retweeted_status\",\n",
    "        \"retweet_count\",\n",
    "        \"favorite_count\",\n",
    "        \"entities\",\n",
    "        \"extended_entities\",\n",
    "        \"possibly_sensitive\",\n",
    "        \"lang\",\n",
    "    ],\n",
    "    # Replace NaNs by empty strings to facilitate pre-processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status = user_data[\n",
    "    [\"id\", \"followers_count\", \"following_count\", \"tweet_count\"]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status[\"timeline_lowest_id\"] = -1\n",
    "user_data_pull_status[\"timeline_tweets_pulled\"] = -1\n",
    "\n",
    "user_data_pull_status[\"followers_cursor\"] = -1\n",
    "user_data_pull_status[\"followers_pulled\"] = -1\n",
    "\n",
    "user_data_pull_status[\"following_cursor\"] = -1\n",
    "user_data_pull_status[\"following_pulled\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status = user_data_pull_status[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"timeline_lowest_id\",\n",
    "        \"timeline_tweets_pulled\",\n",
    "        \"tweet_count\",\n",
    "        \"followers_cursor\",\n",
    "        \"followers_pulled\",\n",
    "        \"followers_count\",\n",
    "        \"following_cursor\",\n",
    "        \"following_pulled\",\n",
    "        \"following_count\",\n",
    "    ]\n",
    "]\n",
    "user_data_pull_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = (\n",
    "    \"https://api.twitter.com/1.1/application/rate_limit_status.json\"\n",
    "    #     + \"783214,15994119,1320117356\"\n",
    "    #     + \"&\"\n",
    "    #     + API_USER_FIELDS\n",
    "    #     + \"&\"\n",
    "    #     + API_TWEET_FIELDS\n",
    ")\n",
    "payload = {\"resources\": \"application,statuses,followers,friends\"}\n",
    "print(req)\n",
    "r = requests.get(req, headers=headers, params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()[\"resources\"][\"statuses\"][\"/statuses/user_timeline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = API_USER_TIMELINE_ENDPOINT + \"?user_id=783214\" + \"&count=10\"\n",
    "print(req)\n",
    "r = requests.get(req, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_user_timeline_df(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.created_at = pd.to_datetime(test.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values(by=\"id\").user[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(r.headers[\"x-rate-limit-remaining\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_reset(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    r.json()[\"data\"],\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"username\",\n",
    "        \"name\",\n",
    "        \"protected\",\n",
    "        \"withheld\",\n",
    "        \"verified\",\n",
    "        \"created_at\",\n",
    "        \"location\",\n",
    "        \"public_metrics\",\n",
    "        \"description\",\n",
    "        \"url\",\n",
    "        \"entities\",\n",
    "        \"pinned_tweet_id\",\n",
    "    ],\n",
    ").fillna(\"\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.public_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "783214 in user_data.id.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_cols = [\"timeline_lowest_id\", \"timeline_tweets_pulled\", \"tweet_count\"]\n",
    "other_cols = [x for x in user_data_pull_status.columns if x not in timeline_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status_timeline = user_data_pull_status[timeline_cols].copy()\n",
    "user_data_pull_status_timeline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status_timeline.to_csv(\n",
    "    DATA_FOLDER + \"public_users_pull_status_timeline.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status_ff = user_data_pull_status[other_cols].copy()\n",
    "user_data_pull_status_ff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status_ff.to_csv(DATA_FOLDER + \"public_users_pull_status_ff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status_timeline.join(user_data_pull_status_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_pull_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(\n",
    "    ~np.equal(\n",
    "        user_data_pull_status.values,\n",
    "        user_data_pull_status_timeline.join(user_data_pull_status_ff).values,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_timeline_data.user_id.value_counts()\n",
    "# user_timeline_data.iloc[1024864]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = user_timeline_data[user_timeline_data.user_id == 1256677638]\n",
    "eval(\n",
    "    user_timeline_data[\n",
    "        ~user_timeline_data.retweeted_status.isna()\n",
    "    ].retweeted_status.iloc[0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2020-ada",
   "language": "python",
   "name": "2020-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
