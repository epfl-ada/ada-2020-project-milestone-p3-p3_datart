{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative extension analysis notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Authors**\n",
    "\n",
    "- Jérémy Bensoussan\n",
    "- Ekaterina Kryukova\n",
    "- Jules Triomphe\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "While the paper examines the exposure hypothesis for all topics, we propose to analyze and compare political and food related tweets. To do so, we plan to obtain egos, alters and their timelines from Twitter’s API by generating $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$. Then, we intend to identify egos’ retweets using official RT, classify tweets by topics based on hashtags (and keywords if the dataset is lacking content) and create a follower/followee graph. To calculate the probability of retweeting alters’ tweets by egos we will use a more solid approach then what is described in the paper “Differences in the Mechanics of Information Diffusion Across Topics” where the probability is equal to the number of users that were k times exposed to a hashtag and retweeted before the ($k+1$)-th exposure, divided by the number of users that were k-times exposed to the hashtag. Finally, we plan to visualize results as well as analyze the probability by breaking down users based on betweenness, clustering coefficient and number of followees.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. Is there a significant difference between the probability of retweeting when the tweet is about food and when it is about politics ?\n",
    "2. Is there a significant difference in the number of times a tweet is retweeted depending on whether it is about food or politics ?\n",
    "3. Is there a relation between user betweenness, size of cluster or number of friends with the retweet probabilities ?\n",
    "\n",
    "## Proposed dataset\n",
    "\n",
    "Self-collected (with the Twitter API) ego and alter timelines with all tweet fields from the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) and all user fields except for `profile_image_url`.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data collection\n",
    "\n",
    "We will sign up for Twitter’s API to collect data. We will generate $3\\times30,000$ random numbers in a range from $0$ to $3,000,000,000$ as in the paper and use the [**GET /2/users** endpoint](https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users) to collect active and public user information with all tweet fields and all user fields except for profile_image_url.\n",
    "\n",
    "### Building the network\n",
    "\n",
    "We will use networkx to build a directed network of followers  in which nodes are users (egos and alters) and edges are the following relationships (without the following relationships among alters). Next, we will build another network where relationships among alters of active egos are included.\n",
    "\n",
    "### Calculating retweet probability\n",
    "\n",
    "For each ego, we will count the number of followees who have retweeted a post (exposures) on a certain topic at a certain date. We will get the information like this: an ego $i$ was exposed to $200$ posts about this topic only once, among which $i$ retweeted $50$ (probability is $50/200 = 25\\%$); at the same time, $i$ was exposed to $100$ posts about this topic twice, among which $i$ retweeted $50$ ($probability = 50\\%$); … Finally, we will calculate a sequence of probability for each ego. (Same procedure as in the paper.) If there is sufficient data, we will apply a t-test to identify whether the distribution of retweets for each exposure count is significantly different from one topic to the other. We will also try to compare the distribution of retweet probabilities between topics.\n",
    "\n",
    "### Community detection\n",
    "\n",
    "We will use the second ego networks to compute clustering coefficients and betweenness of the active egos.\n",
    "\n",
    "### Data analysis\n",
    "\n",
    "We will compare the retweet probabilities based on betweenness, number of followers and clustering for each topic, much like in *Figure 6* in the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d8f9ab97aa40>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-formatting\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# UserID range\n",
    "LOWER_ID_N = 0\n",
    "UPPER_ID_N = int(3e9)\n",
    "\n",
    "# UserID number\n",
    "N_UID_PER_REQUEST = int(3e4)\n",
    "N_UID_REQUESTS = 3\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Choose whether to generate new UserIDs\n",
    "CREATE_NEW_UIDs = False\n",
    "\n",
    "# Choose whether to collect user data\n",
    "COLLECT_USER_DATA = False\n",
    "# Select the batch to query if collecting user data\n",
    "REQUEST_NUMBER = 2\n",
    "# Define behaviour depending on the run number\n",
    "FIRST_RUN = False\n",
    "\n",
    "# Chooser whether to create user subset files\n",
    "CREATE_USER_SUBSETS = True\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Data folder location\n",
    "DATA_FOLDER = \"./data/\"\n",
    "# UIDs file name\n",
    "UIDS_FILE = DATA_FOLDER + \"uids.csv\"\n",
    "USERS_FILE = DATA_FOLDER + \"users.csv\"\n",
    "PUBLIC_USERS_FILE = DATA_FOLDER + \"public_users.csv\"\n",
    "PUBLIC_USERS_W_TWEETS_FILE = DATA_FOLDER + \"public_users_w_tweets.csv\"\n",
    "PUBLIC_USERS_W_FOLLOWERS_FILE = DATA_FOLDER + \"public_users_w_followers.csv\"\n",
    "PUBLIC_USERS_W_FOLLOWEES_FILE = DATA_FOLDER + \"public_users_w_followees.csv\"\n",
    "# File containing the bearer token\n",
    "BEARER_TOKEN = DATA_FOLDER + \"bearer_token.auth\"\n",
    "\n",
    "# API endpoints\n",
    "API_USERS_ENDPOINT = \"https://api.twitter.com/2/users?ids=\"\n",
    "API_USER_FIELDS = \"user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,protected,public_metrics,url,username,verified,withheld\"\n",
    "API_TWEET_FIELDS = \"tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,non_public_metrics,public_metrics,organic_metrics,promoted_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "API_USER_TIMELINE_ENDPOINT = (\n",
    "    \"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id=\"\n",
    ")\n",
    "\n",
    "# Random seed\n",
    "SEED = 30\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data collection\n",
    "\n",
    "In this part, we will generate random user IDs and collect their respective user information if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate UIDs\n",
    "if CREATE_NEW_UIDs:\n",
    "    uids = pd.DataFrame(\n",
    "        np.array(\n",
    "            random.sample(\n",
    "                range(LOWER_ID_N, UPPER_ID_N), N_UID_PER_REQUEST * N_UID_REQUESTS\n",
    "            )\n",
    "        ).reshape(N_UID_PER_REQUEST, N_UID_REQUESTS)\n",
    "    )\n",
    "    uids.to_csv(UIDS_FILE, index=False)\n",
    "else:\n",
    "    uids = pd.read_csv(UIDS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bearer token\n",
    "with open(BEARER_TOKEN, \"r\") as file:\n",
    "    token = file.readline().strip(\"\\n\")\n",
    "\n",
    "# Define authentication header\n",
    "headers = {\"Authorization\": \"Bearer \" + token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248f788adb3d4c70920bb7caef7e90b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def wait_for_reset(r):\n",
    "    print(\n",
    "        \"Current time: {} (UTC)\".format(datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    # Get reset time (Unix format)\n",
    "    ts = int(r.headers[\"x-rate-limit-reset\"])\n",
    "    ts_str = datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Compute difference between current time and reset time\n",
    "    sleep_time = (datetime.fromtimestamp(ts) - datetime.utcnow()).total_seconds()\n",
    "    if sleep_time > 0:\n",
    "        print(\"Waiting until {} (UTC) for the rate limit to reset.\".format(ts_str))\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        print(\"Reset time was: {} (UTC)\".format(ts_str))\n",
    "    print(\"Resuming user data collection\")\n",
    "\n",
    "\n",
    "def get_user_data(req, headers=headers, wait=True):\n",
    "    # Query the user data\n",
    "    r = requests.get(req, headers=headers)\n",
    "    if wait & (int(r.headers[\"x-rate-limit-remaining\"]) == 0):\n",
    "        wait_for_reset(r)\n",
    "\n",
    "        # Query the user data\n",
    "        r = requests.get(req, headers=headers)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_user_data_df(r):\n",
    "    df = pd.DataFrame(\n",
    "        r.json()[\"data\"],\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"username\",\n",
    "            \"name\",\n",
    "            \"protected\",\n",
    "            \"withheld\",\n",
    "            \"verified\",\n",
    "            \"created_at\",\n",
    "            \"location\",\n",
    "            \"public_metrics\",\n",
    "            \"description\",\n",
    "            \"url\",\n",
    "            \"entities\",\n",
    "            \"pinned_tweet_id\",\n",
    "        ],\n",
    "        # Replace NaNs by empty strings to facilitate pre-processing\n",
    "    ).fillna(\"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Get user data\n",
    "if COLLECT_USER_DATA:\n",
    "    for i in trange(N_UID_PER_REQUEST // 100):\n",
    "        # Get 100 UserIDs (limit per request as defined by Twitter)\n",
    "        users = uids.values[i * 100 : (i + 1) * 100, REQUEST_NUMBER]\n",
    "        # Define the request URL\n",
    "        req = (\n",
    "            API_USERS_ENDPOINT\n",
    "            + \",\".join([str(user) for user in users])\n",
    "            + \"&\"\n",
    "            + API_USER_FIELDS\n",
    "            + \"&\"\n",
    "            + API_TWEET_FIELDS\n",
    "        )\n",
    "\n",
    "        # Create the dataframe on the first iteration\n",
    "        if i == 0:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers)\n",
    "\n",
    "            # If the rate limit is not maximal, then wait for the reset to occur\n",
    "            # (max 15 minutes)\n",
    "            if int(r.headers[\"x-rate-limit-remaining\"]) != 299:\n",
    "                wait_for_reset(r)\n",
    "\n",
    "                # Query the user data\n",
    "                r = get_user_data(req, headers=headers)\n",
    "\n",
    "            raw_user_data = get_user_data_df(r)\n",
    "        # Append to existing dataframe on other iterations\n",
    "        # but do not wait for reset for the last iteration\n",
    "        elif i == 299:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers, wait=False)\n",
    "            # Append new data to existing dataframe\n",
    "            raw_user_data = raw_user_data.append(get_user_data_df(r))\n",
    "        else:\n",
    "            # Query the user data\n",
    "            r = get_user_data(req, headers=headers)\n",
    "            # Append new data to existing dataframe\n",
    "            raw_user_data = raw_user_data.append(get_user_data_df(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid users: 11,172\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "\n",
    "def get_key_val(x, key):\n",
    "    \"\"\"Get dictionary value from key if it exists, otherwise return an empty string.\"\"\"\n",
    "    if key in x:\n",
    "        return x[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_public_metrics(df):\n",
    "    \"\"\"Extract the data from the public_metrics column\"\"\"\n",
    "    for metric in [\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]:\n",
    "        df[metric] = df.public_metrics.apply(lambda x: get_key_val(x, metric))\n",
    "    df.pop(\"public_metrics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_entities(df):\n",
    "    \"\"\"Extract the data from the entity column\"\"\"\n",
    "    for entity in [\"url\", \"description\"]:\n",
    "        df[\"entities_\" + entity] = df.entities.apply(lambda x: get_key_val(x, entity))\n",
    "    df.pop(\"entities\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    raw_user_data = get_public_metrics(raw_user_data)\n",
    "    raw_user_data = get_entities(raw_user_data)\n",
    "    raw_user_data = raw_user_data.astype(\n",
    "        {\"id\": int, \"protected\": bool, \"verified\": bool}\n",
    "    )\n",
    "    print(\"Number of valid users: {:,}\".format(raw_user_data.shape[0]))\n",
    "    raw_user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid users: 33,520\n",
      "Number of public accounts: 31,239\n",
      "--> With at least one tweet: 17,406\n"
     ]
    }
   ],
   "source": [
    "if FIRST_RUN:\n",
    "    user_data = raw_user_data\n",
    "else:\n",
    "    user_data = pd.read_csv(\n",
    "        USERS_FILE,\n",
    "        dtype={\"id\": int, \"protected\": bool, \"verified\": bool},\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "    if COLLECT_USER_DATA:\n",
    "        user_data = user_data.append(raw_user_data)\n",
    "\n",
    "if COLLECT_USER_DATA:\n",
    "    # Save data to disk\n",
    "    user_data.to_csv(USERS_FILE, index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Total number of valid users: {:,}\".format(user_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of public users: 31,239\n",
      "Number of public users with tweets: 17,406\n",
      "Number of public users with followers: 18,689\n",
      "Number of public users with followees: 22,099\n"
     ]
    }
   ],
   "source": [
    "# Extract user subsets\n",
    "if CREATE_USER_SUBSETS:\n",
    "    # Public users\n",
    "    public_users = user_data[~user_data.protected].copy()\n",
    "    # Public users with tweets\n",
    "    # Tweet count includes retweets\n",
    "    public_users_w_tweets = user_data[\n",
    "        ~user_data.protected & (user_data.tweet_count > 0)\n",
    "    ].copy()\n",
    "    # Public users with followers\n",
    "    public_users_w_followers = user_data[\n",
    "        ~user_data.protected & (user_data.followers_count > 0)\n",
    "    ].copy()\n",
    "    # Public users with followees\n",
    "    public_users_w_followees = user_data[\n",
    "        ~user_data.protected & (user_data.following_count > 0)\n",
    "    ].copy()\n",
    "\n",
    "    print(\"Number of public users: {:,}\".format(public_users.shape[0]))\n",
    "    print(\n",
    "        \"Number of public users with tweets: {:,}\".format(\n",
    "            public_users_w_tweets.shape[0]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Number of public users with followers: {:,}\".format(\n",
    "            public_users_w_followers.shape[0]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Number of public users with followees: {:,}\".format(\n",
    "            public_users_w_followees.shape[0]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    public_users.to_csv(PUBLIC_USERS_FILE, index=False)\n",
    "    public_users_w_tweets.to_csv(PUBLIC_USERS_W_TWEETS_FILE, index=False)\n",
    "    public_users_w_followers.to_csv(PUBLIC_USERS_W_FOLLOWERS_FILE, index=False)\n",
    "    public_users_w_followees.to_csv(PUBLIC_USERS_W_FOLLOWEES_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-9-6aaf1f276005>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = (\n",
    "    API_USERS_ENDPOINT\n",
    "    + \"783214,15994119,1320117356\"\n",
    "    + \"&\"\n",
    "    + API_USER_FIELDS\n",
    "    + \"&\"\n",
    "    + API_TWEET_FIELDS\n",
    ")\n",
    "print(req)\n",
    "r = requests.get(req, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(r.headers[\"x-rate-limit-remaining\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_reset(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    r.json()[\"data\"],\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"username\",\n",
    "        \"name\",\n",
    "        \"protected\",\n",
    "        \"withheld\",\n",
    "        \"verified\",\n",
    "        \"created_at\",\n",
    "        \"location\",\n",
    "        \"public_metrics\",\n",
    "        \"description\",\n",
    "        \"url\",\n",
    "        \"entities\",\n",
    "        \"pinned_tweet_id\",\n",
    "    ],\n",
    ").fillna(\"\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2020-ada",
   "language": "python",
   "name": "2020-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
